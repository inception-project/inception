// Licensed to the Technische Universität Darmstadt under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The Technische Universität Darmstadt 
// licenses this file to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.
//  
// http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

[[sect_evaluation]]
= Evaluation Simulation

The evaluation simulation panel provides a visualization of the performance of the selected recommender with the help of a learning curve diagram. On the bottom right of the panel, the start button performs evaluation on the selected recommender using the annotated documents in the project and plots the evaluation scores against the training data size on the graph. The evaluation score can be one of the four metrics, Accuracy, Precision, Recall and F1. There is a drop down panel to change the metric. The evaluation might take a long time.

The training data use for the evaluation can be selected using the *Annotator* dropdown. Here,
you can select to train on the annotations of a specific user. Selecting *INITIAL_CAS* trains on
annotations present in the imported original documents. Selecting *CURATION_USER* trains on curated
documents. The data is split into *80% training data* and *20% test data*. The system tries to split the training data in 10 blocks of roughly the same size. For each training run, an additional block
is added to the training data for that run until in the last run, all training data is used.

image::evaluation.png[align="center"]
